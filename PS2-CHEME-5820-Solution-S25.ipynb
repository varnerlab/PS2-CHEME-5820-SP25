{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2017de7-07b5-4d18-ba73-17e2d7767409",
   "metadata": {},
   "source": [
    "# PS2: Logistic Regression Classification of a Clinical Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee3ba1-1b62-41f8-9d0f-075854147d4f",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our lab problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af6d93f-875f-4a42-b5b4-854eef732a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b460c4-34db-4b40-88b5-80396ff6c0fb",
   "metadata": {},
   "source": [
    "### Data\n",
    "Next, let's load up the dataset that we will explore. The data for this lab was taken from this `2020` publication:\n",
    "* [Davide Chicco, Giuseppe Jurman: \"Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone.\" BMC Medical Informatics and Decision Making 20, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5](https://pubmed.ncbi.nlm.nih.gov/32013925/)\n",
    "\n",
    "In this paper, the authors analyzed a dataset of `299` heart failure patients collected in 2015. The patients comprised 105 women and 194 men, aged between 40 and 95 years old. The dataset contains `13` features (a mixture of continuous/categorical data and the label), which report clinical, body, and lifestyle information:\n",
    "* Some features are binary: anemia, high blood pressure, diabetes, sex, and smoking status.\n",
    "* The remaining features were continuous biochemical measurements, such as the level of the Creatinine phosphokinase (CPK) enzyme in the blood, the number of platelets, etc.\n",
    "* The class (target) variable is encoded as a binary (boolean) death event: `1` if the patient died during the follow-up period, `0` if the patient did not die during the follow-up period.\n",
    "\n",
    "We'll load this dataset as a [DataFrame instance](https://dataframes.juliadata.org/stable/) and store it in the `originaldataset::DataFrame` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa87b0d9-6eb2-428f-95ec-0ff35c461323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>299×13 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">274 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">age</th><th style = \"text-align: left;\">anaemia</th><th style = \"text-align: left;\">creatinine_phosphokinase</th><th style = \"text-align: left;\">diabetes</th><th style = \"text-align: left;\">ejection_fraction</th><th style = \"text-align: left;\">high_blood_pressure</th><th style = \"text-align: left;\">platelets</th><th style = \"text-align: left;\">serum_creatinine</th><th style = \"text-align: left;\">serum_sodium</th><th style = \"text-align: left;\">sex</th><th style = \"text-align: left;\">smoking</th><th style = \"text-align: left;\">time</th><th style = \"text-align: left;\">death_event</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">75.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">582</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">265000.0</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: right;\">130</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">55.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7861</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">263358.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">136</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">146</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">162000.0</td><td style = \"text-align: right;\">1.3</td><td style = \"text-align: right;\">129</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">50.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">111</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">210000.0</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: right;\">137</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">160</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">327000.0</td><td style = \"text-align: right;\">2.7</td><td style = \"text-align: right;\">116</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">90.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">47</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">40</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">204000.0</td><td style = \"text-align: right;\">2.1</td><td style = \"text-align: right;\">132</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">75.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">246</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">15</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">127000.0</td><td style = \"text-align: right;\">1.2</td><td style = \"text-align: right;\">137</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">60.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">315</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">60</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">454000.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">131</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">157</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">65</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">263358.0</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">138</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">80.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">123</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">388000.0</td><td style = \"text-align: right;\">9.4</td><td style = \"text-align: right;\">133</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">75.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">81</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">368000.0</td><td style = \"text-align: right;\">4.0</td><td style = \"text-align: right;\">131</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">62.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">231</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">25</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">253000.0</td><td style = \"text-align: right;\">0.9</td><td style = \"text-align: right;\">140</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">981</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">30</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">136000.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">137</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">11</td><td style = \"text-align: right;\">1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">288</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">582</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">55</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">543000.0</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">132</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">250</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">289</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">892</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">263358.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">142</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">256</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">290</td><td style = \"text-align: right;\">90.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">337</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">390000.0</td><td style = \"text-align: right;\">0.9</td><td style = \"text-align: right;\">144</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">256</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">291</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">615</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">55</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">222000.0</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">141</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">257</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">292</td><td style = \"text-align: right;\">60.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">320</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">133000.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">139</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">258</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">293</td><td style = \"text-align: right;\">52.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">190</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">382000.0</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">140</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">258</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">294</td><td style = \"text-align: right;\">63.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">103</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">179000.0</td><td style = \"text-align: right;\">0.9</td><td style = \"text-align: right;\">136</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">270</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">295</td><td style = \"text-align: right;\">62.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">61</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">155000.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">143</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">270</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">296</td><td style = \"text-align: right;\">55.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1820</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">270000.0</td><td style = \"text-align: right;\">1.2</td><td style = \"text-align: right;\">139</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">271</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">297</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2060</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">60</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">742000.0</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">138</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">278</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">298</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2413</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">140000.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">140</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">280</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">299</td><td style = \"text-align: right;\">50.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">196</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">45</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">395000.0</td><td style = \"text-align: right;\">1.6</td><td style = \"text-align: right;\">136</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">285</td><td style = \"text-align: right;\">0</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& age & anaemia & creatinine\\_phosphokinase & diabetes & ejection\\_fraction & high\\_blood\\_pressure & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 75.0 & 0 & 582 & 0 & 20 & 1 & $\\dots$ \\\\\n",
       "\t2 & 55.0 & 0 & 7861 & 0 & 38 & 0 & $\\dots$ \\\\\n",
       "\t3 & 65.0 & 0 & 146 & 0 & 20 & 0 & $\\dots$ \\\\\n",
       "\t4 & 50.0 & 1 & 111 & 0 & 20 & 0 & $\\dots$ \\\\\n",
       "\t5 & 65.0 & 1 & 160 & 1 & 20 & 0 & $\\dots$ \\\\\n",
       "\t6 & 90.0 & 1 & 47 & 0 & 40 & 1 & $\\dots$ \\\\\n",
       "\t7 & 75.0 & 1 & 246 & 0 & 15 & 0 & $\\dots$ \\\\\n",
       "\t8 & 60.0 & 1 & 315 & 1 & 60 & 0 & $\\dots$ \\\\\n",
       "\t9 & 65.0 & 0 & 157 & 0 & 65 & 0 & $\\dots$ \\\\\n",
       "\t10 & 80.0 & 1 & 123 & 0 & 35 & 1 & $\\dots$ \\\\\n",
       "\t11 & 75.0 & 1 & 81 & 0 & 38 & 1 & $\\dots$ \\\\\n",
       "\t12 & 62.0 & 0 & 231 & 0 & 25 & 1 & $\\dots$ \\\\\n",
       "\t13 & 45.0 & 1 & 981 & 0 & 30 & 0 & $\\dots$ \\\\\n",
       "\t14 & 50.0 & 1 & 168 & 0 & 38 & 1 & $\\dots$ \\\\\n",
       "\t15 & 49.0 & 1 & 80 & 0 & 30 & 1 & $\\dots$ \\\\\n",
       "\t16 & 82.0 & 1 & 379 & 0 & 50 & 0 & $\\dots$ \\\\\n",
       "\t17 & 87.0 & 1 & 149 & 0 & 38 & 0 & $\\dots$ \\\\\n",
       "\t18 & 45.0 & 0 & 582 & 0 & 14 & 0 & $\\dots$ \\\\\n",
       "\t19 & 70.0 & 1 & 125 & 0 & 25 & 1 & $\\dots$ \\\\\n",
       "\t20 & 48.0 & 1 & 582 & 1 & 55 & 0 & $\\dots$ \\\\\n",
       "\t21 & 65.0 & 1 & 52 & 0 & 25 & 1 & $\\dots$ \\\\\n",
       "\t22 & 65.0 & 1 & 128 & 1 & 30 & 1 & $\\dots$ \\\\\n",
       "\t23 & 68.0 & 1 & 220 & 0 & 35 & 1 & $\\dots$ \\\\\n",
       "\t24 & 53.0 & 0 & 63 & 1 & 60 & 0 & $\\dots$ \\\\\n",
       "\t25 & 75.0 & 0 & 582 & 1 & 30 & 1 & $\\dots$ \\\\\n",
       "\t26 & 80.0 & 0 & 148 & 1 & 38 & 0 & $\\dots$ \\\\\n",
       "\t27 & 95.0 & 1 & 112 & 0 & 40 & 1 & $\\dots$ \\\\\n",
       "\t28 & 70.0 & 0 & 122 & 1 & 45 & 1 & $\\dots$ \\\\\n",
       "\t29 & 58.0 & 1 & 60 & 0 & 38 & 0 & $\\dots$ \\\\\n",
       "\t30 & 82.0 & 0 & 70 & 1 & 30 & 0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m299×13 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m age     \u001b[0m\u001b[1m anaemia \u001b[0m\u001b[1m creatinine_phosphokinase \u001b[0m\u001b[1m diabetes \u001b[0m\u001b[1m ejection_fraction\u001b[0m ⋯\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m Int64                    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m Int64            \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │    75.0        0                       582         0                 20 ⋯\n",
       "   2 │    55.0        0                      7861         0                 38\n",
       "   3 │    65.0        0                       146         0                 20\n",
       "   4 │    50.0        1                       111         0                 20\n",
       "   5 │    65.0        1                       160         1                 20 ⋯\n",
       "   6 │    90.0        1                        47         0                 40\n",
       "   7 │    75.0        1                       246         0                 15\n",
       "   8 │    60.0        1                       315         1                 60\n",
       "   9 │    65.0        0                       157         0                 65 ⋯\n",
       "  10 │    80.0        1                       123         0                 35\n",
       "  11 │    75.0        1                        81         0                 38\n",
       "  ⋮  │    ⋮        ⋮                ⋮                 ⋮              ⋮         ⋱\n",
       " 290 │    90.0        1                       337         0                 38\n",
       " 291 │    45.0        0                       615         1                 55 ⋯\n",
       " 292 │    60.0        0                       320         0                 35\n",
       " 293 │    52.0        0                       190         1                 38\n",
       " 294 │    63.0        1                       103         1                 35\n",
       " 295 │    62.0        0                        61         1                 38 ⋯\n",
       " 296 │    55.0        0                      1820         0                 38\n",
       " 297 │    45.0        0                      2060         1                 60\n",
       " 298 │    45.0        0                      2413         0                 38\n",
       " 299 │    50.0        0                       196         0                 45 ⋯\n",
       "\u001b[36m                                                  8 columns and 278 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originaldataset = CSV.read(joinpath(_PATH_TO_DATA, \"heart_failure_clinical_records_dataset.csv\"), DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41015c9a-7b7e-4bb2-859c-a8fe35cbe0f5",
   "metadata": {},
   "source": [
    "#### Data rangling\n",
    "We know from lecture that our classification algorithms work on [`Matrix` instance](https://docs.julialang.org/en/v1/base/arrays/#Base.Matrix-Tuple{UndefInitializer,%20Any,%20Any}) and not [`DataFrame` instances](https://dataframes.juliadata.org/stable/). Thus, we need to convert the data to [a `Matrix`](https://docs.julialang.org/en/v1/base/arrays/#Base.Matrix-Tuple{UndefInitializer,%20Any,%20Any}). In addition, there are several ways we can pretreat the data to make the classification easier.\n",
    "* Convert `0,1` data to `-1,1`. This is a preference (not technically required), but it makes (binary) classification problems easier, so let's convert all categorical `0,1` data to `-1,1`. In this rescaled data, `0` will be replaced by `-1`. Thus, _false_ (no death event) will be mapped to `-1`, and _true_ (death event) will remain `1`.\n",
    "* Next, let's [z-score center](https://en.wikipedia.org/wiki/Feature_scaling) the continous feature data. In [z-score feature scaling](https://en.wikipedia.org/wiki/Feature_scaling), we subtract off the mean of each feature and then divide by the standard deviation, i.e., $x^{\\prime} = (x - \\mu)/\\sigma$ where $x$ is the unscaled data, and $x^{\\prime}$ is the scaled data. Under this scaling regime, $x^{\\prime}\\leq{0}$ will be values that are less than or equal to the mean value $\\mu$, while $x^{\\prime}>0$ indicate values that are greater than the mean. The range of data is measured in quanta of the standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd46d13-6580-4513-84a1-dfa25cb2ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(D, dataset) = let\n",
    "\n",
    "    # convert 0,1 into -1,1\n",
    "    treated_dataset = copy(originaldataset);\n",
    "    transform!(treated_dataset, :anaemia => ByRow(x -> (x==0 ? -1 : 1)) => :anaemia); # maps anaemia to -1,1\n",
    "    transform!(treated_dataset, :diabetes => ByRow(x -> (x==0 ? -1 : 1)) => :diabetes); # maps diabetes to -1,1\n",
    "    transform!(treated_dataset, :high_blood_pressure => ByRow(x -> (x==0 ? -1 : 1)) => :high_blood_pressure); # maps high_blood_pressure to -1,1\n",
    "    transform!(treated_dataset, :sex => ByRow(x -> (x==0 ? -1 : 1)) => :sex); # maps sex to -1,1\n",
    "    transform!(treated_dataset, :smoking => ByRow(x -> (x==0 ? -1 : 1)) => :smoking); # maps smoking to -1,1\n",
    "    transform!(treated_dataset, :death_event => ByRow(x -> (x==0 ? -1 : 1)) => :death_event); # maps death_event to -1,1\n",
    "    \n",
    "    D = treated_dataset[:,1:end] |> Matrix; # build a data matrix from the DataFrame\n",
    "    (number_of_examples, number_of_features) = size(D);\n",
    "\n",
    "    # Which cols do we want to rescale?\n",
    "    index_to_z_scale = [\n",
    "        1 ; # 1 age\n",
    "        3 ; # 2 creatinine_phosphokinase\n",
    "        5 ; # 3 ejection_fraction\n",
    "        7 ; # 4 platelets\n",
    "        8 ; # 5 serum_creatinine\n",
    "        9 ; # 6 serum_sodium\n",
    "        12 ; # 7 time\n",
    "    ];\n",
    "\n",
    "    D̂ = copy(D);\n",
    "    for i ∈ eachindex(index_to_z_scale)\n",
    "        j = index_to_z_scale[i];\n",
    "        μ = mean(D[:,j]); # compute the mean\n",
    "        σ = std(D[:,j]); # compute std\n",
    "\n",
    "        # rescale -\n",
    "        for k ∈ 1:number_of_examples\n",
    "            D̂[k,j] = (D[k,j] - μ)/σ;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    D̂, treated_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd2cbb4-f040-4580-85b9-1f311d184739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299×13 Matrix{Float64}:\n",
       "  1.19095    -1.0   0.000165451  -1.0  …   1.0  -1.0  -1.62678   1.0\n",
       " -0.490457   -1.0   7.50206      -1.0      1.0  -1.0  -1.60101   1.0\n",
       "  0.350246   -1.0  -0.449186     -1.0      1.0   1.0  -1.58812   1.0\n",
       " -0.910808    1.0  -0.485257     -1.0      1.0  -1.0  -1.58812   1.0\n",
       "  0.350246    1.0  -0.434757      1.0     -1.0  -1.0  -1.57524   1.0\n",
       "  2.452       1.0  -0.551217     -1.0  …   1.0   1.0  -1.57524   1.0\n",
       "  1.19095     1.0  -0.346124     -1.0      1.0  -1.0  -1.54947   1.0\n",
       " -0.0701056   1.0  -0.275011      1.0      1.0   1.0  -1.54947   1.0\n",
       "  0.350246   -1.0  -0.437849     -1.0     -1.0  -1.0  -1.54947   1.0\n",
       "  1.6113      1.0  -0.47289      -1.0      1.0   1.0  -1.54947   1.0\n",
       "  1.19095     1.0  -0.516176     -1.0  …   1.0   1.0  -1.54947   1.0\n",
       "  0.098035   -1.0  -0.361583     -1.0      1.0   1.0  -1.54947   1.0\n",
       " -1.33116     1.0   0.411384     -1.0      1.0  -1.0  -1.53659   1.0\n",
       "  ⋮                                    ⋱         ⋮              \n",
       " -1.33116    -1.0   0.000165451   1.0     -1.0  -1.0   1.54275  -1.0\n",
       "  0.350246   -1.0   0.319658      1.0     -1.0  -1.0   1.62005  -1.0\n",
       "  2.452       1.0  -0.252337     -1.0     -1.0  -1.0   1.62005  -1.0\n",
       " -1.33116    -1.0   0.034176      1.0  …  -1.0  -1.0   1.63294  -1.0\n",
       " -0.0701056  -1.0  -0.269858     -1.0      1.0  -1.0   1.64582  -1.0\n",
       " -0.742668   -1.0  -0.403838      1.0      1.0   1.0   1.64582  -1.0\n",
       "  0.182105    1.0  -0.493502      1.0      1.0   1.0   1.80043  -1.0\n",
       "  0.098035   -1.0  -0.536789      1.0      1.0   1.0   1.80043  -1.0\n",
       " -0.490457   -1.0   1.27608      -1.0  …  -1.0  -1.0   1.81332  -1.0\n",
       " -1.33116    -1.0   1.52342       1.0     -1.0  -1.0   1.90351  -1.0\n",
       " -1.33116    -1.0   1.88723      -1.0      1.0   1.0   1.92927  -1.0\n",
       " -0.910808   -1.0  -0.397655     -1.0      1.0   1.0   1.9937   -1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de29e32-61b6-422e-b73b-6c75f3a46e78",
   "metadata": {},
   "source": [
    "Next, let's split that dataset `D` into `training` and `test` subsets. We do this randomly, where the `number_of_training_examples::Int64` variable specifies the number of training points. The `training::Array{Float64,2}` data will be used to estimate the model parameters, and `test::Array{Float64,2}` will be used for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f4d216-30b1-4b0e-9bf2-afafda98e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_training_examples = 199; # set the number to set the number of training examples\n",
    "    number_of_examples = size(D,1); # number of rows in the full dataset\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4612af4-eade-4614-be19-f0a60b757dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199×13 Matrix{Float64}:\n",
       "  2.87235     1.0  -0.217296     -1.0  …   1.0  -1.0  -1.0341      1.0\n",
       "  0.350246   -1.0  -0.502778      1.0      1.0  -1.0  -1.30467     1.0\n",
       "  0.938738   -1.0  -0.22451       1.0      1.0   1.0  -0.918142    1.0\n",
       " -1.58337     1.0  -0.342001      1.0     -1.0  -1.0  -0.840837    1.0\n",
       "  0.350246    1.0  -0.460523     -1.0      1.0  -1.0   0.82123    -1.0\n",
       "  2.03165    -1.0   5.46246      -1.0  …   1.0   1.0  -0.750647    1.0\n",
       " -0.910808   -1.0   1.99957      -1.0     -1.0  -1.0   1.07891    -1.0\n",
       " -0.0701056   1.0   0.177432      1.0      1.0  -1.0  -0.505846   -1.0\n",
       " -0.490457   -1.0  -0.537819     -1.0      1.0   1.0  -0.518731   -1.0\n",
       " -0.238246    1.0  -0.450216     -1.0      1.0   1.0   0.512008    1.0\n",
       " -0.154176   -1.0  -0.531635      1.0  …   1.0  -1.0   0.0610601   1.0\n",
       "  0.350246    1.0  -0.333756      1.0      1.0  -1.0   1.34948     1.0\n",
       "  0.350246   -1.0   0.000165451   1.0      1.0   1.0   1.05315    -1.0\n",
       "  ⋮                                    ⋱         ⋮                \n",
       "  0.350246   -1.0  -0.483196      1.0      1.0  -1.0  -0.815068    1.0\n",
       "  0.350246   -1.0  -0.541942     -1.0     -1.0  -1.0   0.988725   -1.0\n",
       "  0.182105    1.0   0.000165451  -1.0      1.0   1.0  -0.0935508  -1.0\n",
       "  0.266176    1.0  -0.535758     -1.0  …  -1.0  -1.0   0.563545   -1.0\n",
       "  1.02281    -1.0   0.000165451  -1.0      1.0  -1.0   0.872767    1.0\n",
       " -0.0701056  -1.0   3.48573       1.0     -1.0  -1.0  -1.12429     1.0\n",
       "  0.350246   -1.0  -0.395593      1.0      1.0   1.0   0.202787   -1.0\n",
       "  0.350246   -1.0  -0.449186     -1.0      1.0   1.0  -1.58812     1.0\n",
       " -0.994879    1.0  -0.528544     -1.0  …  -1.0  -1.0   0.215671   -1.0\n",
       "  1.19095     1.0   0.000165451  -1.0      1.0  -1.0  -0.222393    1.0\n",
       " -1.75151    -1.0  -0.107019      1.0      1.0  -1.0   0.228555   -1.0\n",
       " -1.33116    -1.0   1.88723      -1.0      1.0   1.0   1.92927    -1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee6163-bb20-4a12-b3a0-de7f48ade08e",
   "metadata": {},
   "source": [
    "## Task 1: Build a Perceptron Classification Model and Learn the Parameters\n",
    "In this task, we'll build a model of our classification problem and train the model using an online learning method. \n",
    "* __Training__: Our Perceptron implementation [based on pseudo-code](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) stores problem information in [a `MyPerceptronClassificationModel` instance, which holds the (initial) parameters and other data](src/Types.jl) required by the problem. We initialize the parameters using a vector of `1`'s.\n",
    "* Next, we then _learn_ the model parameters [using the `learn(...)` method](src/Compute.jl), which takes the training features array `X,` the training labels vector `y`, and the problem instance and returns an updated problem instance holding the updated parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9799fa3c-c140-41ef-8d68-90fb1f15d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 1000. We have number of errors: 59\n"
     ]
    }
   ],
   "source": [
    "perceptron_model = let\n",
    "    \n",
    "    # setup\n",
    "    D = training; # what dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features, what??\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "    \n",
    "    # build an initial model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features),\n",
    "        mistakes = 0 # willing to live with m mistakes\n",
    "    ));\n",
    "\n",
    "    # TODO: uncomment me to train the model -\n",
    "    trainedmodel = learn(X,y,model, maxiter = 1000, verbose = true);\n",
    "\n",
    "    # return -\n",
    "    trainedmodel;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bbe51-b486-4ffd-aad0-c9b4305aecc9",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between classes on data it has never seen. \n",
    "* We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the estimated labels. We store the actual (correct) label in the `y_perceptron::Array{Int64,1}` vector, while the model predicted label is stored in the `ŷ_perceptron::Array{Int64,1}` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63371d2d-9810-458b-99f4-3125a50b7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_perceptron,y_perceptron = let\n",
    "\n",
    "    D = test; # what dataset are going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    ŷ = classify(X,perceptron_model)\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf6e83-62ad-462d-bd4a-13158cce17dc",
   "metadata": {},
   "source": [
    "__Confusion matrix__: The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf). \n",
    "\n",
    "Let's compute the confusion matrix [using the `confusion(...)` method](src/Compute.jl) and store it in the `CM_perceptron::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d97a103a-9b9c-476f-8145-2dec340799b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 27   5\n",
       " 15  53"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_perceptron = confusion(y_perceptron, ŷ_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51a2c0-5833-4879-a24e-09d95eef41e1",
   "metadata": {},
   "source": [
    "Finally, we can compute the overall error rate for the perceptron (or other performance metrics) using values from [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "114c9a1c-bd9a-4f5b-83d9-ad5d10dd5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.8 Fraction incorrect 0.19999999999999996\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_perceptron = CM_perceptron[1,1] + CM_perceptron[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c598b7-ec09-4903-8026-c9310702797f",
   "metadata": {},
   "source": [
    "## Task 2: Build and Train Logistic Regression Classification Model using Gradient Descent\n",
    "In this task, we build and train a [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier using the training data, and then challenge this classifier using the `test` dataset. We'll use [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) for parameter estimation.\n",
    "\n",
    "We implemented [the `MyLogisticRegressionClassificationModel` type](src/Types.jl), which contains data required to solve the logistic regression problem, i.e., parameters, the learning rate, a stopping tolerance parameter $\\epsilon$, and a loss (objective) function that we want to minimize. \n",
    "* __Technical note__: We approximated the gradient calculation using [a forward finite difference](https://en.wikipedia.org/wiki/Finite_difference). This is generally not a great idea. This is one of my super pet peeves with gradient descent; computing the gradient is (usually) a hassle. Typically, we must do at least two function evaluations to approximate the gradient well. Why do finite diference? It is easy to implement.\n",
    "* In the code below, we [build a `model::MyLogisticRegressionClassificationModel` instance using a `build(...)` method](src/Factory.jl). The model instance initially has a random guess for the classifier parameters. We use gradient descent to refine that guess [using the `learn(...)` method](src/Compute.jl), which returns an updated model instance (with the best parameters that we found so far). We return the updated model instance and save it in the `model_logistic::MyLogisticRegressionClassificationModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7047099c-2637-4a03-b000-f10c306a772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 318. We have error: 9.833191494922282e-5\n"
     ]
    }
   ],
   "source": [
    "model_logistic = let\n",
    "\n",
    "    # data -\n",
    "    D = training; # What dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionClassificationModel, (\n",
    "        parameters = 0.01*ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        learning_rate = 0.01, # you pick this\n",
    "        ϵ = 1e-4, # you pick this (this is also the step size for the fd approx to the gradient)\n",
    "        loss_function = (x,y,θ) -> log10(1+exp(-y*(dot(x,θ)))) # what??!? Wow, that is nice. Yes, we can pass functions as args!\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 10000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94596661-c975-4804-8cf1-78d7b51a206d",
   "metadata": {},
   "source": [
    "Let's use the updated `model_logistic::MyLogisticRegressionClassificationModel` instance (with parameters learned from the `training` data) and test how well we can classify data in the `test` dataset.\n",
    "\n",
    "* __Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the probability of a label in the `P::Array{Float64,2}` array (which is different than the Perceptron). Each row of `P` corresponds to a test instance, in which each column corresponds to a label, in the case `1` and `-1`.\n",
    "* We store the actual (correct) label in the `y_logistic::Array{Int64,1}` vector. We compute the predicted label for each test instance by finding the highest probability column. We store the predicted labels in the `ŷ_logistic::Array{Int64,1}` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a5c92b1-ff26-4b4f-8d52-6acf4f9471ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_logistic,y_logistic, P = let\n",
    "\n",
    "    D = test; # What dataset are you going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    P = classify(X,model_logistic) # logistic regression returns a x x 2 array holding the probability\n",
    "\n",
    "    # convert the probability to a choice ... for each row (test instance), compute the col with the highest probability\n",
    "    ŷ = zeros(number_of_examples);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        a = argmax(P[i,:]); # col index with largest value\n",
    "        ŷ[i] = 1; # default\n",
    "        if (a == 2)\n",
    "            ŷ[i] = -1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    ŷ, y, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667f8cb-c8cc-4142-9347-18a9df24375f",
   "metadata": {},
   "source": [
    "__Performance__: Once we have has converged (or exhasted our iterations), we can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the `test` dataset and measure wins (when the label is the same) and losses (label is different). This is easily represented in [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "* We compute confusion matrix [using the `confusion(...)` method](src/Compute.jl) and store it in the `CM_logistic::Array{Int64,2}` variable. The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef0211cb-f382-4a5b-98b8-9660125027ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 23   9\n",
       "  4  64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_logistic = confusion(y_logistic, ŷ_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d4eb6-e8a3-4b0a-861f-1cbac829bc93",
   "metadata": {},
   "source": [
    "Let's compute the overall error rate for the logistic regression using [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97d1d3d6-f28e-4907-9d67-4c9d088a0d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.87 Fraction incorrect 0.13\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_logistic = CM_logistic[1,1] + CM_logistic[2,2];\n",
    "(correct_prediction_logistic/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79722cf-3752-4a85-b26d-947b5cc49d14",
   "metadata": {},
   "source": [
    "## Task 3: Build and Train Logistic Regression Classification Model using Simulated Annealing\n",
    "In this task, we build and train a [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier using the training data, and then challenge this classifier using the `test` dataset. We'll use [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing#:~:text=Simulated%20annealing%20(SA)%20is%20a,can%20find%20the%20global%20optimum.) for parameter estimation.\n",
    "\n",
    "We implemented [the `MyLogisticRegressionClassificationModel` type](src/Types.jl), which contains data required to solve the logistic regression problem, i.e., parameters, the learning rate, a stopping tolerance parameter $\\epsilon$, and a loss (objective) function that we want to minimize. \n",
    "* __Technical note__: We approximated the gradient calculation using [a forward finite difference](https://en.wikipedia.org/wiki/Finite_difference). This is generally not a great idea. This is one of my super pet peeves with gradient descent; computing the gradient is (usually) a hassle. Typically, we must do at least two function evaluations to approximate the gradient well. Why do finite diference? It is easy to implement.\n",
    "* In the code below, we [build a `model::MyLogisticRegressionClassificationModel` instance using a `build(...)` method](src/Factory.jl). The model instance initially has a random guess for the classifier parameters. We use gradient descent to refine that guess [using the `learn(...)` method](src/Compute.jl), which returns an updated model instance (with the best parameters that we found so far). We return the updated model instance and save it in the `model_logistic::MyLogisticRegressionClassificationModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac9856-bc70-45de-a091-cef25af17288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
