{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e151e7",
   "metadata": {},
   "source": [
    "# Algorithm: Simplified Gradient Descent\n",
    "Gradient descent minimizes a function by iteratively adjusting the parameters in the _opposite direction_ of the gradient. \n",
    "> The gradient is a vector that points in the direction of the steepest increase of the function, and its magnitude indicates how steep the slope is in that direction. Thus, the negative gradient points in the direction of the steepest decrease of the function.\n",
    "\n",
    "For Logistic Regression, let's examine a simple algorithm for estimating the parameters $\\theta$ that appear in the negative log-likelihood function $L(\\theta)$ using gradient descent.\n",
    "\n",
    "__Initialization:__ We start with an initial guess for the parameters $\\theta^{(0)}\\in\\mathbb{R}^{p}$, which can be a random vector or a vector of zeros. Specify a learning rate $\\alpha(k)>0$ (which can be constant or a function of the iteration count $k$) and a stopping criterion, e.g., a small threshold $\\epsilon>0$ and a maximum number of iterations `maxiter`. Set $\\texttt{converged}\\gets\\texttt{false}$ and the iteration count $k\\gets0$.\n",
    "\n",
    "While not $\\texttt{converged}$ __do__:\n",
    "1. Compute the update direction $\\mathbf{d} = -\\nabla_{\\theta}L(\\theta^{(k)})$, where $\\nabla_{\\theta}L(\\theta^{(k)})$ is the gradient of the loss function with respect to the parameters at iteration $k$.\n",
    "2. Update the parameters $\\theta^{(k+1)} = \\theta^{(k)} + \\alpha(k)\\cdot\\mathbf{d}$.\n",
    "3. Check the stopping criterion:\n",
    "   - If $\\lVert\\theta^{(k+1)} - \\theta^{(k)}\\rVert\\leq\\epsilon$, set $\\texttt{converged} = \\texttt{true}$. Alternatively, if the gradient is small at the current iteration, i.e., $\\lVert\\nabla_{\\theta}L(\\theta^{(k)})\\rVert\\leq\\epsilon$, set $\\texttt{converged} = \\texttt{true}$.\n",
    "   - If $k\\geq\\texttt{maxiter}$, set $\\texttt{converged} = \\texttt{true}$.\n",
    "     > __Optional__:\n",
    "     >  Warn the user that the maximum number of iterations has been reached _without convergence_.\n",
    "   - If neither of the above conditions is met, continue iterating.\n",
    "4. Increment the iteration count $k\\gets k+1$, and update the learning rate $\\alpha(k)$, if applicable.\n",
    "\n",
    "\n",
    "Let's take a closer look at some of the key components of this algorithm:\n",
    "* __What is $\\alpha(k)$?__ The (hyper) parameter $\\alpha(k)>0$ is the _learning rate_ which can be a function of the iteration count $k$. This is a user-adjustable parameter, and we'll assume it's constant for today. Alternatively, we can have a mechanism to adjust the learning rate. The learning rate controls how much we adjust the parameters at each iteration. If it's too large, we may overshoot the minimum; if it's too small, convergence may be slow.\n",
    "* __Stopping?__ Gradient descent will continue to iterate until a stopping criterion is met, i.e., $\\lVert\\theta^{(k+1)} - \\theta^{(k)}\\rVert\\leq\\epsilon$ or the maximum number of iterations is reached, or some other stopping criterion is met, i.e., the gradient is small at the current iteration $\\lVert\\nabla_{\\theta}{L}(\\theta^{(k)})\\rVert\\leq\\epsilon$. Here, we have a choice in defining the stopping criterion, such as which component to monitor or which norm to use. The choice of stopping criterion can affect the convergence of the algorithm.\n",
    "* __Alternatives to gradient descent?__ Yes, computing the gradient is a _hassle_. There are alternatives to gradient descent that evaluate the objective function directly, such as the [Nelder-Mead Simplex Algorithm](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), [Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing), [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm), [Particle Swarm Optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization), etc. We can use these gradient-free alternatives to estimate model parameters without relying on the gradient.\n",
    "\n",
    "**How good is gradient descent?**\n",
    "\n",
    "Like most things in life, it depends. It depends on the problem, the learning rate you choose, and your stopping rule. In practice, gradient descent works very well for logistic regression, thanks to the convex, single-minimum shape of the loss, but can struggle on non-convex losses or if you pick a rate that’s too large or too small.\n",
    "\n",
    "* **A smooth, bowl-shaped loss:** The logistic loss is like a gently sloping bowl with exactly one minimum. Its gradient can’t change too abruptly, so if you pick a step size up to about $1/L$, where $L$ is the Lipschitz constant of the gradient, you’re guaranteed to keep rolling down toward that unique bottom.\n",
    "* **Slowing down over time:** Initially, you make large improvements in the loss, but each subsequent step helps a bit less than the last. After $k$ steps, your gap to the optimal loss shrinks on the order of $1/k$. In other words, you sprint at first, then settle into a steady jog.\n",
    "* **Adding regularization speeds things up:** If you add $l_{2}$ (ridge) regularization, the loss becomes strongly convex. The bowl gets uniformly steeper, and each gradient descent step reduces the remaining error by a constant fraction, leading to exponentially fast convergence.\n",
    "* **Picking the proper rate:** The _sweet spot_ for the learning rate is roughly $\\alpha\\approx 2/(L+\\mu)$, where $\\mu$ is the strong-convexity constant (the minimum curvature at the bottom). But even the simpler choice $\\alpha=1/L$ still guarantees a steady, reliable decrease each iteration.\n",
    "\n",
    "Several techniques can accelerate convergence beyond basic gradient descent:\n",
    "\n",
    "> __Advanced Optimization Techniques:__\n",
    "> \n",
    "> * **Diminishing learning rates:** Decrease the learning rate over time (e.g., $\\alpha_k = \\alpha_0/k$) to ensure convergence when the optimal step size is unknown. Early iterations take larger steps, while later iterations make finer adjustments.\n",
    "> * **Momentum methods:** Build velocity by accumulating gradients from previous iterations, helping to smooth oscillations and accelerate convergence, especially when the loss surface is poorly conditioned or has flat regions.\n",
    "> * **Second-order methods:** Newton's method uses curvature information (the Hessian matrix) to take more informed steps, achieving faster convergence near the minimum but requiring additional computation for second derivatives.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7dafd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
