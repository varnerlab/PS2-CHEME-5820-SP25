{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2017de7-07b5-4d18-ba73-17e2d7767409",
   "metadata": {},
   "source": [
    "# PS2: Logistic Regression Classification of a Clinical Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee3ba1-1b62-41f8-9d0f-075854147d4f",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our lab problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af6d93f-875f-4a42-b5b4-854eef732a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b460c4-34db-4b40-88b5-80396ff6c0fb",
   "metadata": {},
   "source": [
    "### Data\n",
    "Next, let's load up the dataset that we will explore. The data for this lab was taken from this `2020` publication:\n",
    "* [Davide Chicco, Giuseppe Jurman: \"Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone.\" BMC Medical Informatics and Decision Making 20, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5](https://pubmed.ncbi.nlm.nih.gov/32013925/)\n",
    "\n",
    "In this paper, the authors analyzed a dataset of `299` heart failure patients collected in 2015. The patients comprised 105 women and 194 men, aged between 40 and 95 years old. The dataset contains `13` features (a mixture of continuous/categorical data and the label), which report clinical, body, and lifestyle information:\n",
    "* Some features are binary: anemia, high blood pressure, diabetes, sex, and smoking status.\n",
    "* The remaining features were continuous biochemical measurements, such as the level of the Creatinine phosphokinase (CPK) enzyme in the blood, the number of platelets, etc.\n",
    "* The class (target) variable is encoded as a binary (boolean) death event: `1` if the patient died during the follow-up period, `0` if the patient did not die during the follow-up period.\n",
    "\n",
    "We'll load this dataset as a [DataFrame instance](https://dataframes.juliadata.org/stable/) and store it in the `originaldataset::DataFrame` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa87b0d9-6eb2-428f-95ec-0ff35c461323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>299×13 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">274 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">age</th><th style = \"text-align: left;\">anaemia</th><th style = \"text-align: left;\">creatinine_phosphokinase</th><th style = \"text-align: left;\">diabetes</th><th style = \"text-align: left;\">ejection_fraction</th><th style = \"text-align: left;\">high_blood_pressure</th><th style = \"text-align: left;\">platelets</th><th style = \"text-align: left;\">serum_creatinine</th><th style = \"text-align: left;\">serum_sodium</th><th style = \"text-align: left;\">sex</th><th style = \"text-align: left;\">smoking</th><th style = \"text-align: left;\">time</th><th style = \"text-align: left;\">death_event</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">75.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">582</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">265000.0</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: right;\">130</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">55.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7861</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">263358.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">136</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">146</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">162000.0</td><td style = \"text-align: right;\">1.3</td><td style = \"text-align: right;\">129</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">50.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">111</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">210000.0</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: right;\">137</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">160</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">20</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">327000.0</td><td style = \"text-align: right;\">2.7</td><td style = \"text-align: right;\">116</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">90.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">47</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">40</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">204000.0</td><td style = \"text-align: right;\">2.1</td><td style = \"text-align: right;\">132</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">75.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">246</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">15</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">127000.0</td><td style = \"text-align: right;\">1.2</td><td style = \"text-align: right;\">137</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">60.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">315</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">60</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">454000.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">131</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">157</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">65</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">263358.0</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">138</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">80.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">123</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">388000.0</td><td style = \"text-align: right;\">9.4</td><td style = \"text-align: right;\">133</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">75.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">81</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">368000.0</td><td style = \"text-align: right;\">4.0</td><td style = \"text-align: right;\">131</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">62.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">231</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">25</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">253000.0</td><td style = \"text-align: right;\">0.9</td><td style = \"text-align: right;\">140</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">981</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">30</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">136000.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">137</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">11</td><td style = \"text-align: right;\">1</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">288</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">582</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">55</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">543000.0</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">132</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">250</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">289</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">892</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">263358.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">142</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">256</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">290</td><td style = \"text-align: right;\">90.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">337</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">390000.0</td><td style = \"text-align: right;\">0.9</td><td style = \"text-align: right;\">144</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">256</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">291</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">615</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">55</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">222000.0</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">141</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">257</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">292</td><td style = \"text-align: right;\">60.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">320</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">133000.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">139</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">258</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">293</td><td style = \"text-align: right;\">52.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">190</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">382000.0</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">140</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">258</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">294</td><td style = \"text-align: right;\">63.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">103</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">179000.0</td><td style = \"text-align: right;\">0.9</td><td style = \"text-align: right;\">136</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">270</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">295</td><td style = \"text-align: right;\">62.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">61</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">155000.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">143</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">270</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">296</td><td style = \"text-align: right;\">55.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1820</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">270000.0</td><td style = \"text-align: right;\">1.2</td><td style = \"text-align: right;\">139</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">271</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">297</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2060</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">60</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">742000.0</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">138</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">278</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">298</td><td style = \"text-align: right;\">45.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2413</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">140000.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">140</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">280</td><td style = \"text-align: right;\">0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">299</td><td style = \"text-align: right;\">50.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">196</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">45</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">395000.0</td><td style = \"text-align: right;\">1.6</td><td style = \"text-align: right;\">136</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">285</td><td style = \"text-align: right;\">0</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& age & anaemia & creatinine\\_phosphokinase & diabetes & ejection\\_fraction & high\\_blood\\_pressure & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Int64 & Int64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 75.0 & 0 & 582 & 0 & 20 & 1 & $\\dots$ \\\\\n",
       "\t2 & 55.0 & 0 & 7861 & 0 & 38 & 0 & $\\dots$ \\\\\n",
       "\t3 & 65.0 & 0 & 146 & 0 & 20 & 0 & $\\dots$ \\\\\n",
       "\t4 & 50.0 & 1 & 111 & 0 & 20 & 0 & $\\dots$ \\\\\n",
       "\t5 & 65.0 & 1 & 160 & 1 & 20 & 0 & $\\dots$ \\\\\n",
       "\t6 & 90.0 & 1 & 47 & 0 & 40 & 1 & $\\dots$ \\\\\n",
       "\t7 & 75.0 & 1 & 246 & 0 & 15 & 0 & $\\dots$ \\\\\n",
       "\t8 & 60.0 & 1 & 315 & 1 & 60 & 0 & $\\dots$ \\\\\n",
       "\t9 & 65.0 & 0 & 157 & 0 & 65 & 0 & $\\dots$ \\\\\n",
       "\t10 & 80.0 & 1 & 123 & 0 & 35 & 1 & $\\dots$ \\\\\n",
       "\t11 & 75.0 & 1 & 81 & 0 & 38 & 1 & $\\dots$ \\\\\n",
       "\t12 & 62.0 & 0 & 231 & 0 & 25 & 1 & $\\dots$ \\\\\n",
       "\t13 & 45.0 & 1 & 981 & 0 & 30 & 0 & $\\dots$ \\\\\n",
       "\t14 & 50.0 & 1 & 168 & 0 & 38 & 1 & $\\dots$ \\\\\n",
       "\t15 & 49.0 & 1 & 80 & 0 & 30 & 1 & $\\dots$ \\\\\n",
       "\t16 & 82.0 & 1 & 379 & 0 & 50 & 0 & $\\dots$ \\\\\n",
       "\t17 & 87.0 & 1 & 149 & 0 & 38 & 0 & $\\dots$ \\\\\n",
       "\t18 & 45.0 & 0 & 582 & 0 & 14 & 0 & $\\dots$ \\\\\n",
       "\t19 & 70.0 & 1 & 125 & 0 & 25 & 1 & $\\dots$ \\\\\n",
       "\t20 & 48.0 & 1 & 582 & 1 & 55 & 0 & $\\dots$ \\\\\n",
       "\t21 & 65.0 & 1 & 52 & 0 & 25 & 1 & $\\dots$ \\\\\n",
       "\t22 & 65.0 & 1 & 128 & 1 & 30 & 1 & $\\dots$ \\\\\n",
       "\t23 & 68.0 & 1 & 220 & 0 & 35 & 1 & $\\dots$ \\\\\n",
       "\t24 & 53.0 & 0 & 63 & 1 & 60 & 0 & $\\dots$ \\\\\n",
       "\t25 & 75.0 & 0 & 582 & 1 & 30 & 1 & $\\dots$ \\\\\n",
       "\t26 & 80.0 & 0 & 148 & 1 & 38 & 0 & $\\dots$ \\\\\n",
       "\t27 & 95.0 & 1 & 112 & 0 & 40 & 1 & $\\dots$ \\\\\n",
       "\t28 & 70.0 & 0 & 122 & 1 & 45 & 1 & $\\dots$ \\\\\n",
       "\t29 & 58.0 & 1 & 60 & 0 & 38 & 0 & $\\dots$ \\\\\n",
       "\t30 & 82.0 & 0 & 70 & 1 & 30 & 0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m299×13 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m age     \u001b[0m\u001b[1m anaemia \u001b[0m\u001b[1m creatinine_phosphokinase \u001b[0m\u001b[1m diabetes \u001b[0m\u001b[1m ejection_fraction\u001b[0m ⋯\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m Int64                    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m Int64            \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │    75.0        0                       582         0                 20 ⋯\n",
       "   2 │    55.0        0                      7861         0                 38\n",
       "   3 │    65.0        0                       146         0                 20\n",
       "   4 │    50.0        1                       111         0                 20\n",
       "   5 │    65.0        1                       160         1                 20 ⋯\n",
       "   6 │    90.0        1                        47         0                 40\n",
       "   7 │    75.0        1                       246         0                 15\n",
       "   8 │    60.0        1                       315         1                 60\n",
       "   9 │    65.0        0                       157         0                 65 ⋯\n",
       "  10 │    80.0        1                       123         0                 35\n",
       "  11 │    75.0        1                        81         0                 38\n",
       "  ⋮  │    ⋮        ⋮                ⋮                 ⋮              ⋮         ⋱\n",
       " 290 │    90.0        1                       337         0                 38\n",
       " 291 │    45.0        0                       615         1                 55 ⋯\n",
       " 292 │    60.0        0                       320         0                 35\n",
       " 293 │    52.0        0                       190         1                 38\n",
       " 294 │    63.0        1                       103         1                 35\n",
       " 295 │    62.0        0                        61         1                 38 ⋯\n",
       " 296 │    55.0        0                      1820         0                 38\n",
       " 297 │    45.0        0                      2060         1                 60\n",
       " 298 │    45.0        0                      2413         0                 38\n",
       " 299 │    50.0        0                       196         0                 45 ⋯\n",
       "\u001b[36m                                                  8 columns and 278 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originaldataset = CSV.read(joinpath(_PATH_TO_DATA, \"heart_failure_clinical_records_dataset.csv\"), DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41015c9a-7b7e-4bb2-859c-a8fe35cbe0f5",
   "metadata": {},
   "source": [
    "#### Data rangling\n",
    "We know from lecture that our classification algorithms work on [`Matrix` instance](https://docs.julialang.org/en/v1/base/arrays/#Base.Matrix-Tuple{UndefInitializer,%20Any,%20Any}) and not [`DataFrame` instances](https://dataframes.juliadata.org/stable/). Thus, we need to convert the data to [a `Matrix`](https://docs.julialang.org/en/v1/base/arrays/#Base.Matrix-Tuple{UndefInitializer,%20Any,%20Any}). In addition, there are several ways we can pretreat the data to make the classification easier.\n",
    "* Convert `0,1` data to `-1,1`. This is a preference (not technically required), but it makes (binary) classification problems easier, so let's convert all categorical `0,1` data to `-1,1`. In this rescaled data, `0` will be replaced by `-1`. Thus, _false_ (no death event) will be mapped to `-1`, and _true_ (death event) will remain `1`.\n",
    "* Next, let's [z-score center](https://en.wikipedia.org/wiki/Feature_scaling) the continous feature data. In [z-score feature scaling](https://en.wikipedia.org/wiki/Feature_scaling), we subtract off the mean of each feature and then divide by the standard deviation, i.e., $x^{\\prime} = (x - \\mu)/\\sigma$ where $x$ is the unscaled data, and $x^{\\prime}$ is the scaled data. Under this scaling regime, $x^{\\prime}\\leq{0}$ will be values that are less than or equal to the mean value $\\mu$, while $x^{\\prime}>0$ indicate values that are greater than the mean. The range of data is measured in quanta of the standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd46d13-6580-4513-84a1-dfa25cb2ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(D, dataset) = let\n",
    "\n",
    "    # convert 0,1 into -1,1\n",
    "    treated_dataset = copy(originaldataset);\n",
    "    transform!(treated_dataset, :anaemia => ByRow(x -> (x==0 ? -1 : 1)) => :anaemia); # maps anaemia to -1,1\n",
    "    transform!(treated_dataset, :diabetes => ByRow(x -> (x==0 ? -1 : 1)) => :diabetes); # maps diabetes to -1,1\n",
    "    transform!(treated_dataset, :high_blood_pressure => ByRow(x -> (x==0 ? -1 : 1)) => :high_blood_pressure); # maps high_blood_pressure to -1,1\n",
    "    transform!(treated_dataset, :sex => ByRow(x -> (x==0 ? -1 : 1)) => :sex); # maps sex to -1,1\n",
    "    transform!(treated_dataset, :smoking => ByRow(x -> (x==0 ? -1 : 1)) => :smoking); # maps smoking to -1,1\n",
    "    transform!(treated_dataset, :death_event => ByRow(x -> (x==0 ? -1 : 1)) => :death_event); # maps death_event to -1,1\n",
    "    \n",
    "    D = treated_dataset[:,1:end] |> Matrix; # build a data matrix from the DataFrame\n",
    "    (number_of_examples, number_of_features) = size(D);\n",
    "\n",
    "    # Which cols do we want to rescale?\n",
    "    index_to_z_scale = [\n",
    "        1 ; # 1 age\n",
    "        3 ; # 2 creatinine_phosphokinase\n",
    "        5 ; # 3 ejection_fraction\n",
    "        7 ; # 4 platelets\n",
    "        8 ; # 5 serum_creatinine\n",
    "        9 ; # 6 serum_sodium\n",
    "        12 ; # 7 time\n",
    "    ];\n",
    "\n",
    "    D̂ = copy(D);\n",
    "    for i ∈ eachindex(index_to_z_scale)\n",
    "        j = index_to_z_scale[i];\n",
    "        μ = mean(D[:,j]); # compute the mean\n",
    "        σ = std(D[:,j]); # compute std\n",
    "\n",
    "        # rescale -\n",
    "        for k ∈ 1:number_of_examples\n",
    "            D̂[k,j] = (D[k,j] - μ)/σ;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    D̂, treated_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd2cbb4-f040-4580-85b9-1f311d184739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299×13 Matrix{Float64}:\n",
       "  1.19095    -1.0   0.000165451  -1.0  …   1.0  -1.0  -1.62678   1.0\n",
       " -0.490457   -1.0   7.50206      -1.0      1.0  -1.0  -1.60101   1.0\n",
       "  0.350246   -1.0  -0.449186     -1.0      1.0   1.0  -1.58812   1.0\n",
       " -0.910808    1.0  -0.485257     -1.0      1.0  -1.0  -1.58812   1.0\n",
       "  0.350246    1.0  -0.434757      1.0     -1.0  -1.0  -1.57524   1.0\n",
       "  2.452       1.0  -0.551217     -1.0  …   1.0   1.0  -1.57524   1.0\n",
       "  1.19095     1.0  -0.346124     -1.0      1.0  -1.0  -1.54947   1.0\n",
       " -0.0701056   1.0  -0.275011      1.0      1.0   1.0  -1.54947   1.0\n",
       "  0.350246   -1.0  -0.437849     -1.0     -1.0  -1.0  -1.54947   1.0\n",
       "  1.6113      1.0  -0.47289      -1.0      1.0   1.0  -1.54947   1.0\n",
       "  1.19095     1.0  -0.516176     -1.0  …   1.0   1.0  -1.54947   1.0\n",
       "  0.098035   -1.0  -0.361583     -1.0      1.0   1.0  -1.54947   1.0\n",
       " -1.33116     1.0   0.411384     -1.0      1.0  -1.0  -1.53659   1.0\n",
       "  ⋮                                    ⋱         ⋮              \n",
       " -1.33116    -1.0   0.000165451   1.0     -1.0  -1.0   1.54275  -1.0\n",
       "  0.350246   -1.0   0.319658      1.0     -1.0  -1.0   1.62005  -1.0\n",
       "  2.452       1.0  -0.252337     -1.0     -1.0  -1.0   1.62005  -1.0\n",
       " -1.33116    -1.0   0.034176      1.0  …  -1.0  -1.0   1.63294  -1.0\n",
       " -0.0701056  -1.0  -0.269858     -1.0      1.0  -1.0   1.64582  -1.0\n",
       " -0.742668   -1.0  -0.403838      1.0      1.0   1.0   1.64582  -1.0\n",
       "  0.182105    1.0  -0.493502      1.0      1.0   1.0   1.80043  -1.0\n",
       "  0.098035   -1.0  -0.536789      1.0      1.0   1.0   1.80043  -1.0\n",
       " -0.490457   -1.0   1.27608      -1.0  …  -1.0  -1.0   1.81332  -1.0\n",
       " -1.33116    -1.0   1.52342       1.0     -1.0  -1.0   1.90351  -1.0\n",
       " -1.33116    -1.0   1.88723      -1.0      1.0   1.0   1.92927  -1.0\n",
       " -0.910808   -1.0  -0.397655     -1.0      1.0   1.0   1.9937   -1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de29e32-61b6-422e-b73b-6c75f3a46e78",
   "metadata": {},
   "source": [
    "Next, let's split that dataset `D` into `training` and `test` subsets. We do this randomly, where the `number_of_training_examples::Int64` variable specifies the number of training points. The `training::Array{Float64,2}` data will be used to estimate the model parameters, and `test::Array{Float64,2}` will be used for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f4d216-30b1-4b0e-9bf2-afafda98e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_training_examples = 199; # set the number to set the number of training examples\n",
    "    number_of_examples = size(D,1); # number of rows in the full dataset\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4612af4-eade-4614-be19-f0a60b757dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199×13 Matrix{Float64}:\n",
       "  0.350246   -1.0  -0.502778      1.0  …   1.0  -1.0  -1.30467     1.0\n",
       "  0.938738   -1.0  -0.22451       1.0      1.0   1.0  -0.918142    1.0\n",
       " -0.490457   -1.0   0.000165451   1.0     -1.0  -1.0   0.859883   -1.0\n",
       "  0.350246    1.0  -0.460523     -1.0      1.0  -1.0   0.82123    -1.0\n",
       "  2.03165    -1.0   5.46246      -1.0      1.0   1.0  -0.750647    1.0\n",
       " -0.910808   -1.0   1.99957      -1.0  …  -1.0  -1.0   1.07891    -1.0\n",
       " -0.0701056   1.0   0.177432      1.0      1.0  -1.0  -0.505846   -1.0\n",
       " -0.490457   -1.0  -0.537819     -1.0      1.0   1.0  -0.518731   -1.0\n",
       " -0.826738   -1.0  -0.519268     -1.0      1.0  -1.0  -0.660457   -1.0\n",
       " -0.154176   -1.0  -0.531635      1.0      1.0  -1.0   0.0610601   1.0\n",
       "  0.350246    1.0  -0.333756      1.0  …   1.0  -1.0   1.34948     1.0\n",
       "  0.350246   -1.0   0.000165451   1.0      1.0   1.0   1.05315    -1.0\n",
       "  0.350246   -1.0  -0.192561      1.0      1.0   1.0   0.305861    1.0\n",
       "  ⋮                                    ⋱         ⋮                \n",
       "  0.350246    1.0  -0.546064     -1.0     -1.0  -1.0  -1.47216    -1.0\n",
       "  0.350246   -1.0  -0.541942     -1.0     -1.0  -1.0   0.988725   -1.0\n",
       "  1.77944     1.0   0.281525      1.0     -1.0  -1.0  -1.29178     1.0\n",
       " -0.0701056  -1.0   3.48573       1.0  …  -1.0  -1.0  -1.12429     1.0\n",
       " -0.826738   -1.0   0.822602     -1.0      1.0  -1.0  -1.18871     1.0\n",
       "  0.350246   -1.0  -0.395593      1.0      1.0   1.0   0.202787   -1.0\n",
       " -0.490457    1.0  -0.424451      1.0      1.0  -1.0   1.54275    -1.0\n",
       " -0.994879    1.0  -0.528544     -1.0     -1.0  -1.0   0.215671   -1.0\n",
       " -1.75151    -1.0  -0.107019      1.0  …   1.0  -1.0   0.228555   -1.0\n",
       "  0.350246    1.0  -0.254398     -1.0     -1.0  -1.0  -0.132203   -1.0\n",
       " -1.33116    -1.0   0.000165451  -1.0     -1.0  -1.0  -0.866605   -1.0\n",
       " -1.33116    -1.0   1.88723      -1.0      1.0   1.0   1.92927    -1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b8e41-9368-4da7-b3ef-0b9a8a8baf65",
   "metadata": {},
   "source": [
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "245f73a8-9d8d-43c2-aa65-de3647837f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12×12 Matrix{Float64}:\n",
       "  1.0         0.0873213  -0.0815839   …   0.0174608   -0.224068\n",
       "  0.0873213   0.98449    -0.189256       -0.0995713   -0.140313\n",
       " -0.0815839  -0.189256    1.0             0.00226468  -0.00934565\n",
       " -0.0998138  -0.0124801  -0.00952414     -0.136024     0.0333253\n",
       "  0.0600984   0.0313113  -0.0440796      -0.0629621    0.0417292\n",
       "  0.0892094   0.0362281  -0.0675033   …  -0.0498305   -0.18785\n",
       " -0.0523544  -0.0434447   0.0244634       0.0264088    0.0105139\n",
       "  0.159187    0.0517674  -0.0164085      -0.0256416   -0.149315\n",
       " -0.0459658   0.0415555   0.0595502       0.00450198   0.08764\n",
       "  0.0625685  -0.0899194   0.0763016       0.398824    -0.0149257\n",
       "  0.0174608  -0.0995713   0.00226468  …   0.874863    -0.0213622\n",
       " -0.224068   -0.140313   -0.00934565     -0.0213622    1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Σ = cov(D[:,1:end-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b77e48-93bd-426a-a12a-821069bbc5e4",
   "metadata": {},
   "source": [
    "Finally, let's set up the color dictionary to visualize the classification datasets. The keys of the `my_color_dictionary::Dict Int64, RGB` dictionary class labels, i.e., $ y\\in\\{1,-1\\}$ while the values are the colors mapped to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81273fbc-a0b6-4e21-ae19-c65015a06bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color_dictionary = Dict{Int64,RGB}();\n",
    "my_color_dictionary[1] = colorant\"#03045e\"; # color for Label = 1\n",
    "my_color_dictionary[-1] = colorant\"#e36414\"; # color for Label = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee6163-bb20-4a12-b3a0-de7f48ade08e",
   "metadata": {},
   "source": [
    "## Task 1: Build a Perceptron Classification Model and Learn the Parameters\n",
    "In this task, we'll build a model of our classification problem and train the model using an online learning method. \n",
    "* __Training__: Our Perceptron implementation [based on pseudo-code](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3a/docs/Notes.pdf) stores problem information in [a `MyPerceptronClassificationModel` instance, which holds the (initial) parameters and other data](src/Types.jl) required by the problem. We initialize the parameters using a vector of `1`'s.\n",
    "* Next, we then _learn_ the model parameters [using the `learn(...)` method](src/Compute.jl), which takes the training features array `X,` the training labels vector `y`, and the problem instance and returns an updated problem instance holding the updated parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9799fa3c-c140-41ef-8d68-90fb1f15d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 1000. We have number of errors: 56\n"
     ]
    }
   ],
   "source": [
    "perceptron_model = let\n",
    "    \n",
    "    # setup\n",
    "    D = training; # what dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features, what??\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "    \n",
    "    # build an initial model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features),\n",
    "        mistakes = 0 # willing to live with m mistakes\n",
    "    ));\n",
    "\n",
    "    # TODO: uncomment me to train the model -\n",
    "    trainedmodel = learn(X,y,model, maxiter = 1000, verbose = true);\n",
    "\n",
    "    # return -\n",
    "    trainedmodel;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bbe51-b486-4ffd-aad0-c9b4305aecc9",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between classes on data it has never seen. \n",
    "* We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the estimated labels. We store the actual (correct) label in the `y_perceptron::Array{Int64,1}` vector, while the model predicted label is stored in the `ŷ_perceptron::Array{Int64,1}` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63371d2d-9810-458b-99f4-3125a50b7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_perceptron,y_perceptron = let\n",
    "\n",
    "    D = test; # what dataset are going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    ŷ = classify(X,perceptron_model)\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf6e83-62ad-462d-bd4a-13158cce17dc",
   "metadata": {},
   "source": [
    "__Confusion matrix__: Let's compute the confusion matrix for the perceptron [using the `confusion(...)` method](src/Compute.jl) and store it in the `CM_perceptron::Array{Int64,2}` variable. [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d97a103a-9b9c-476f-8145-2dec340799b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 25  10\n",
       "  9  56"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_perceptron = confusion(y_perceptron, ŷ_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51a2c0-5833-4879-a24e-09d95eef41e1",
   "metadata": {},
   "source": [
    "Finally, we can compute the overall error rate for the perceptron (or other performance metrics) using values from [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "114c9a1c-bd9a-4f5b-83d9-ad5d10dd5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.81 Fraction incorrect 0.18999999999999995\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_perceptron = CM_perceptron[1,1] + CM_perceptron[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c598b7-ec09-4903-8026-c9310702797f",
   "metadata": {},
   "source": [
    "## Task 2: Build and Train Logistic Regression Classification Model using Gradient Descent\n",
    "In this task, we build and train a [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier using the training data, and then challenge this classifier using the `test` dataset. We'll use [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) for parameter estimation.\n",
    "\n",
    "We implemented [the `MyLogisticRegressionClassificationModel` type](src/Types.jl), which contains data required to solve the logistic regression problem, i.e., parameters, the learning rate, a stopping tolerance parameter $\\epsilon$, and a loss (objective) function that we want to minimize. \n",
    "* __Technical note__: We approximated the gradient calculation using [a forward finite difference](https://en.wikipedia.org/wiki/Finite_difference). This is generally not a great idea. This is one of my super pet peeves with gradient descent; computing the gradient is (usually) a hassle. Typically, we must do at least two function evaluations to approximate the gradient well. Why do finite diference? It is easy to implement.\n",
    "* In the code below, we [build a `model::MyLogisticRegressionClassificationModel` instance using a `build(...)` method](src/Factory.jl). The model instance initially has a random guess for the classifier parameters. We use gradient descent to refine that guess [using the `learn(...)` method](src/Compute.jl), which returns an updated model instance (with the best parameters that we found so far). We return the updated model instance and save it in the `model_logistic::MyLogisticRegressionClassificationModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7047099c-2637-4a03-b000-f10c306a772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped after number of iterations: 10001. We have error: 32.34784690545399\n"
     ]
    }
   ],
   "source": [
    "model_logistic = let\n",
    "\n",
    "    # data -\n",
    "    D = training; # What dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionClassificationModel, (\n",
    "        parameters = 0.01*ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        learning_rate = 0.01, # you pick this\n",
    "        ϵ = 1e-4, # you pick this (this is also the step size for the fd approx to the gradient)\n",
    "        loss_function = (x,y,θ) -> log10(1+exp(-y*(dot(x,θ)))) # what??!? Wow, that is nice. Yes, we can pass functions as args!\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 10000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94596661-c975-4804-8cf1-78d7b51a206d",
   "metadata": {},
   "source": [
    "Let's use the updated `model_logistic::MyLogisticRegressionClassificationModel` instance (with parameters learned from the `training` data) and test how well we can classify data in the `test` dataset.\n",
    "\n",
    "* __Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the probability of a label in the `P::Array{Float64,2}` array (which is different than the Perceptron). Each row of `P` corresponds to a test instance, in which each column corresponds to a label, in the case `1` and `-1`.\n",
    "* We store the actual (correct) label in the `y_logistic::Array{Int64,1}` vector. We compute the predicted label for each test instance by finding the highest probability column. We store the predicted labels in the `ŷ_logistic::Array{Int64,1}` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a5c92b1-ff26-4b4f-8d52-6acf4f9471ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_logistic,y_logistic, P = let\n",
    "\n",
    "    D = test; # What dataset are you going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    P = classify(X,model_logistic) # logistic regression returns a x x 2 array holding the probability\n",
    "\n",
    "    # convert the probability to a choice ... for each row (test instance), compute the col with the highest probability\n",
    "    ŷ = zeros(number_of_examples);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        a = argmax(P[i,:]); # col index with largest value\n",
    "        ŷ[i] = 1; # default\n",
    "        if (a == 2)\n",
    "            ŷ[i] = -1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    ŷ, y, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667f8cb-c8cc-4142-9347-18a9df24375f",
   "metadata": {},
   "source": [
    "__Performance__: Once we have has converged (or exhasted our iterations), we can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the `test` dataset and measure wins (when the label is the same) and losses (label is different). This is easily represented in [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "* We compute confusion matrix [using the `confusion(...)` method](src/Compute.jl) and store it in the `CM_logistic::Array{Int64,2}` variable. The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef0211cb-f382-4a5b-98b8-9660125027ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 26   9\n",
       "  6  59"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_logistic = confusion(y_logistic, ŷ_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d4eb6-e8a3-4b0a-861f-1cbac829bc93",
   "metadata": {},
   "source": [
    "Let's compute the overall error rate for the logistic regression using [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97d1d3d6-f28e-4907-9d67-4c9d088a0d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.85 Fraction incorrect 0.15000000000000002\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_logistic = CM_logistic[1,1] + CM_logistic[2,2];\n",
    "(correct_prediction_logistic/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79722cf-3752-4a85-b26d-947b5cc49d14",
   "metadata": {},
   "source": [
    "## Task 3: Build and Train Logistic Regression Classification Model using Simulated Annealing\n",
    "In this task, we build and train a [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier using the `training` dataset, and then challenge this classifier using the `test` dataset. We'll use [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing#:~:text=Simulated%20annealing%20(SA)%20is%20a,can%20find%20the%20global%20optimum.) for parameter estimation (instead of [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)).\n",
    "\n",
    "Simulated annealing, inspired by the annealing process in metallurgy, is a probabilistic optimization technique used to estimate the global optimum of a given function in large search spaces _without_ computing the gradient. It is particularly effective for problems with numerous local optima. \n",
    "\n",
    "__Algorithm__: The simulated annealing algorithm generates a random parameter set and evaluates its loss function. This loss is compared with the best found so far. If the loss decreases, the candidate set becomes the best. If the loss increases, the candidate set may be kept with a probability tied to a temperature $T$. The pseudocode for simulated annealing can be found in [L3c course notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3c/docs/Notes.pdf).\n",
    "\n",
    "### Implementation\n",
    "\n",
    "* We implemented [the `MyLogisticRegressionSimulatedAnnealingClassificationModel` type](src/Types.jl), which contains data required to train the logistic regression problem using simulated annealing. To build this type, we [use a `build(...)` method](src/Factory.jl) and pass in an initial value for the parameters, a cooling rate parameter (important to the probability of accepting an uphill move), the loss function, etc. \n",
    "* We then estimate the model parameters [using the `learn(...)` method](src/Compute.jl). This method takes the `training` feature matrix $\\hat{\\mathbf{X}}$, the label vector $\\mathbf{y}$, and the model instance, along with some optional parameter, such the `maxiter::Int64` parameters. The `maxiter::Int64` argument has a slightly different meaning in simulated annealing; it's the number of steps we take at each temperature $T$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66ac9856-bc70-45de-a091-cef25af17288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA routine: Stopped with best loss: 32.356797539351746\n"
     ]
    }
   ],
   "source": [
    "model_simulated_annealing = let\n",
    "\n",
    "    # data -\n",
    "    D = training; # What dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionSimulatedAnnealingClassificationModel, (\n",
    "        parameters = ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        cooling_rate = 0.01, # you pick this: the smaller the value, the *slower* we cool.\n",
    "        ϵ = 1e-4, # you pick this (this is also the step size for the fd approx to the gradient)\n",
    "        loss_function = (x,y,θ) -> log10(1+exp(-y*(dot(x,θ)))) # what??!? Wow, that is nice. Yes, we can pass functions as args!\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 100, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c35a6-37e7-4402-aa50-328e8c3bb449",
   "metadata": {},
   "source": [
    "Let's use the updated `model_simulated_annealing::MyLogisticRegressionSimulatedAnnealingClassificationModel` instance (with parameters learned from the `training` data) and test how well we can classify data in the `test` dataset (using simulated annealing instead of gradient descent).\n",
    "\n",
    "* __Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](src/Compute.jl). This method takes a feature array `X` and the (trained) model instance. It returns the probability of a label in the `P::Array{Float64,2}` array (which is different than the Perceptron). Each row of `P` corresponds to a test instance, in which each column corresponds to a label, in the case `1` and `-1`.\n",
    "* We store the actual (correct) label in the `y_logistic_sa::Array{Int64,1}` vector. We compute the predicted label for each test instance by finding the highest probability column. We store the predicted labels in the `ŷ_logistic_sa::Array{Int64,1}` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7261439-ec21-462e-8476-a9ae442eaf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_logistic_sa,y_logistic_sa, P = let\n",
    "\n",
    "    D = test; # What dataset are you going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    P = classify(X,model_simulated_annealing) # logistic regression returns a x x 2 array holding the probability 1 = col 1, -1 = col 2\n",
    "\n",
    "    # convert the probability to a choice ... for each row (test instance), compute the col with the highest probability\n",
    "    ŷ = zeros(number_of_examples);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        a = argmax(P[i,:]); # col index with largest value\n",
    "        ŷ[i] = 1; # default\n",
    "        if (a == 2)\n",
    "            ŷ[i] = -1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    ŷ, y, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ca82a-b515-497f-8f3f-b23da3447b5a",
   "metadata": {},
   "source": [
    "__Performance__: Let's compute [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for the simulated annealing case.\n",
    "* We compute confusion matrix [using the `confusion(...)` method](src/Compute.jl) and store it in the `CM_logistic_sa::Array{Int64,2}` variable. The [`confusion(...)` method](src/Compute.jl) takes the actual labels and the computed labels and returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cec88797-7781-44e0-adca-e8af3755eddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 26   9\n",
       "  6  59"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_logistic_sa = confusion(y_logistic_sa, ŷ_logistic_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c04225b5-f22c-43fb-90e6-09d412373a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.85 Fraction incorrect 0.15000000000000002\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_logistic_sa = CM_logistic_sa[1,1] + CM_logistic_sa[2,2];\n",
    "(correct_prediction_logistic_sa/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15f874-9b19-492e-bb6b-d61b12a3b64c",
   "metadata": {},
   "source": [
    "## Task 4: Let's visualize what we missed using PCA!\n",
    "In L3d, we had a kind of cool visualization of which points our various classifiers missed. Let's try to do the same here. However, we have a problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5497cf2b-0231-4c23-95cc-c757c41a6f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×12 Matrix{Float64}:\n",
       " -0.492007  -0.259916  0.18883   …  -0.145568  -0.0837004  0.499135\n",
       " -0.059407  -0.322449  0.247051      0.579358   0.484378   0.102824"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = let\n",
    "    F = eigen(Σ);\n",
    "    λ = F.values;\n",
    "    V = F.vectors;\n",
    "\n",
    "    # get the largest and next largets eigenvectors\n",
    "    v₁ = V[:,end]; # largest\n",
    "    v₂ = V[:,end-1]; # next largest\n",
    "\n",
    "    # build the projection matrix -\n",
    "    P = [\n",
    "        transpose(v₁) ;\n",
    "        transpose(v₂)\n",
    "    ];\n",
    "\n",
    "    # return -\n",
    "    P\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9717a1c0-9dce-4454-b80c-cb6b1353c00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199×2 Matrix{Float64}:\n",
       " -0.378744  -0.579692\n",
       " -1.11256    1.18376\n",
       "  1.85559   -1.27028\n",
       " -0.448313  -0.16056\n",
       " -0.525391   3.24188\n",
       "  2.1989    -0.123081\n",
       " -0.936017  -0.681483\n",
       " -0.224546   1.8979\n",
       "  1.06482    0.160619\n",
       " -0.558047   1.10854\n",
       " -0.245946   0.382246\n",
       "  0.654841   1.70119\n",
       "  0.271468   1.67965\n",
       "  ⋮         \n",
       " -1.455     -1.36458\n",
       " -1.36046    0.107387\n",
       " -0.568989  -2.60082\n",
       " -0.377279  -0.945909\n",
       " -0.53634    1.02711\n",
       "  0.126468   0.851097\n",
       "  1.13029   -0.305329\n",
       "  0.67227   -1.07147\n",
       "  1.6972     0.661798\n",
       " -0.580629  -1.30722\n",
       "  1.20747   -1.33693\n",
       "  1.95118    2.77718"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = let\n",
    "\n",
    "    D = training; # what data set?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = D[:,1:end-1]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    \n",
    "    Z = P*transpose(X) |> transpose |> Matrix\n",
    "\n",
    "    Z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b98eeef5-5681-4e40-9f02-210188e3d624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×13 Matrix{Float64}:\n",
       "  2.87235     1.0  -0.217296     -1.0  …   1.0  -1.0  -1.0341     1.0\n",
       " -1.33116    -1.0  -0.298715      1.0      1.0   1.0  -0.544499  -1.0\n",
       "  2.452       1.0  -0.551217     -1.0      1.0   1.0  -1.57524    1.0\n",
       " -1.58337     1.0  -0.342001      1.0     -1.0  -1.0  -0.840837   1.0\n",
       " -0.910808   -1.0  -0.481135     -1.0      1.0   1.0  -0.157972  -1.0\n",
       " -0.238246    1.0  -0.450216     -1.0  …   1.0   1.0   0.512008   1.0\n",
       " -0.0140307   1.0  -0.492472      1.0      1.0  -1.0   0.524893   1.0\n",
       "  0.098035   -1.0  -0.310052      1.0     -1.0  -1.0  -0.286814  -1.0\n",
       " -0.910808    1.0  -0.485257     -1.0      1.0  -1.0  -1.58812    1.0\n",
       " -1.75151     1.0  -0.495564     -1.0     -1.0  -1.0   0.731041  -1.0\n",
       " -1.58337    -1.0   4.76885      -1.0  …   1.0   1.0  -0.557383  -1.0\n",
       "  0.686527   -1.0   0.862796     -1.0      1.0   1.0   0.215671  -1.0\n",
       "  0.098035   -1.0  -0.361583     -1.0      1.0   1.0  -1.54947    1.0\n",
       "  ⋮                                    ⋱         ⋮               \n",
       "  1.19095    -1.0   0.0960133     1.0     -1.0  -1.0   0.962957  -1.0\n",
       "  0.266176    1.0  -0.535758     -1.0     -1.0  -1.0   0.563545  -1.0\n",
       "  1.02281    -1.0   0.000165451  -1.0  …   1.0  -1.0   0.872767   1.0\n",
       " -1.41523    -1.0  -0.513084      1.0      1.0  -1.0  -0.660457  -1.0\n",
       "  0.350246   -1.0  -0.449186     -1.0      1.0   1.0  -1.58812    1.0\n",
       " -0.238246    1.0  -0.462584     -1.0      1.0  -1.0  -0.60892   -1.0\n",
       "  0.602457    1.0  -0.00498766   -1.0      1.0  -1.0  -1.12429    1.0\n",
       " -1.58337    -1.0  -0.533697     -1.0  …   1.0  -1.0   1.47833   -1.0\n",
       "  1.19095     1.0   0.000165451  -1.0      1.0  -1.0  -0.222393   1.0\n",
       " -0.0701056  -1.0   0.000165451   1.0      1.0   1.0  -1.16294    1.0\n",
       "  0.182105    1.0  -0.0699168     1.0      1.0  -1.0  -0.60892   -1.0\n",
       " -0.742668   -1.0  -0.403838      1.0      1.0   1.0   1.64582   -1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1284dbc7-9733-430a-a5a2-b877f330200a",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 100-element Vector{Float64} at index [101]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 100-element Vector{Float64} at index [101]",
      "",
      "Stacktrace:",
      " [1] throw_boundserror(A::Vector{Float64}, I::Tuple{Int64})",
      "   @ Base ./essentials.jl:14",
      " [2] getindex(A::Vector{Float64}, i::Int64)",
      "   @ Base ./essentials.jl:916",
      " [3] top-level scope",
      "   @ In[43]:34"
     ]
    }
   ],
   "source": [
    "let\n",
    "    model = perceptron_model; # which model am I using?\n",
    "    dataset = training; # what dataset am I looking at?\n",
    "    caselabel = \"training\";\n",
    "    actual = y_perceptron;\n",
    "    predicted = ŷ_perceptron;\n",
    "    number_of_points = size(dataset,1); # number of rows\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    \n",
    "    # plot label = 1\n",
    "    testlabel = 1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,end])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([Z[i,1]], [Z[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot label = -1\n",
    "    testlabel = -1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,end])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([Z[i,1]], [Z[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # let's draw the separating hyperplane (in our case, a line)\n",
    "    p = model.β;\n",
    "    number_of_plane_points = 200;\n",
    "    x₂ = zeros(number_of_plane_points);\n",
    "    x₁ = range(-3,stop=3,length = number_of_plane_points) |> collect;\n",
    "    for i ∈ 1:number_of_plane_points\n",
    "        x₂[i] = -1*((p[1]/p[2])*x₁[i] + p[3]/p[2]);\n",
    "    end\n",
    "    plot!(x₁,x₂,lw=2, c=:green, label=\"Learned boundary\")\n",
    "    \n",
    "    # data -\n",
    "    for i ∈ 1:number_of_points\n",
    "        actuallabel = actual[i]; # actual label\n",
    "        testlabel = predicted[i]; # predited label\n",
    "\n",
    "        c = :gray60;\n",
    "        if (actuallabel == testlabel)\n",
    "            c = my_color_dictionary[actuallabel]\n",
    "        end\n",
    "        scatter!([Z[i, 1]], [Z[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "    end\n",
    "\n",
    "    title!(\"Perceptron: $(caselabel)\", fontsize=18)\n",
    "    xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "    ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec5c02c-727a-4ddd-8b5c-462be7eed438",
   "metadata": {},
   "source": [
    "## Tests\n",
    "In the code block below, we check some values in your notebook and give you feedback on which items are correct or different. `Unhide` the code block below (if you are curious) about how we implemented the tests and what we are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9490364-804d-4a14-bb01-bd625380cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    # fill me in here\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
